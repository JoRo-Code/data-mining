{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "1. Frequent itemsets with support s\n",
    "2. Association Rules with confidence c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A priori:\n",
    "C1\n",
    "    - read transactions and get counts\n",
    "L1\n",
    "    - filter candidates with support s\n",
    "\n",
    "CK\n",
    "    - generate candidates CK out of L(k-1), with monotonicity support s,\n",
    "    - read transactions and get counts\n",
    "LK\n",
    "- filter candidates LK with support s\n",
    "\n",
    "\n",
    "\n",
    "Keep in mind\n",
    "- if the files are very large and can't be fit into memory, then these files should be split. With a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 100000\n",
      "First few transactions: [{448, 834, 164, 775, 328, 687, 240, 368, 274, 561, 52, 630, 825, 25, 538, 730}, {704, 834, 581, 39, 205, 814, 401, 120, 825, 124}, {674, 35, 712, 854, 759, 950, 249, 733}]\n",
      "Total unique items: 870\n"
     ]
    }
   ],
   "source": [
    "def load_transactions(filepath):\n",
    "    transactions = []\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            # Convert each line to a set of integers\n",
    "            transaction = set(map(int, line.strip().split()))\n",
    "            transactions.append(transaction)\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "transactions = load_transactions('data/T10I4D100K.dat')\n",
    "print(f\"Number of transactions: {len(transactions)}\")\n",
    "print(f\"First few transactions: {transactions[:3]}\")\n",
    "\n",
    "all_items = set().union(*transactions)\n",
    "print(f\"Total unique items: {len(all_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Priori algorithm\n",
    "\n",
    "the idea is to incrementally build larger itemsets by combining frequently occuring smaller subsets.\n",
    "\n",
    "\n",
    "### Frequent singletons:\n",
    "\n",
    "    1. find singletons\n",
    "    2. c1 <- count singletons\n",
    "    3. l1 <- filter frequent c1\n",
    "\n",
    "### Frequent 2-itemsets\n",
    "\n",
    "    1. candidates <- combine singletons to 2-itemsets\n",
    "    2. c2 <- count candidates\n",
    "    3. l2 <- filter c2\n",
    "\n",
    "### Frequent k-itemsets\n",
    "    1. candidates <- combine singletons with k-1 item sets\n",
    "    2. ensure frequent subsets - they occur in all frequently filtered candidates L (consisting of each lk)\n",
    "    3. ck <- count candidates\n",
    "    4. lk <- filter ck\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def A_priori(transactions, s, max_k=3):\n",
    "    # get counts for singletons (k=1)\n",
    "    c1 = {frozenset([item]): 0 for item in all_items}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            c1[frozenset([item])] += 1\n",
    "    \n",
    "    # filter singletons with support s  \n",
    "    l1 = {itemset: count for itemset, count in c1.items() if count >= s}\n",
    "    \n",
    "    # initialize result with L1\n",
    "    L = [l1]\n",
    "    C = [c1]\n",
    "    \n",
    "    print(f\"C1: {len(c1)} - {list(c1)[:5]}..\")\n",
    "    print(f\"L1: {len(l1)} - {list(l1)[:5]}..\")\n",
    "    \n",
    "    # iterate for k=2 to max_k\n",
    "    for k in range(2, max_k + 1):\n",
    "        # generate candidates from previous frequent itemsets\n",
    "        ck = {}\n",
    "        prev_l = L[k-2]\n",
    "        \n",
    "        # generate candidates by combining previous frequent itemsets\n",
    "        for itemset1 in prev_l:\n",
    "            for singleton in l1:\n",
    "                union = itemset1.union(singleton)\n",
    "                if len(union) == k:\n",
    "                    \n",
    "                    # subset frequency check\n",
    "                    all_subsets_frequent = True\n",
    "                    for item in union:\n",
    "                        subset = frozenset(union - {item})\n",
    "                        if subset not in prev_l:\n",
    "                            all_subsets_frequent = False\n",
    "                            break\n",
    "                    \n",
    "                    if all_subsets_frequent:\n",
    "                        ck[union] = 0\n",
    "        \n",
    "        print(f\"C{k} candidates: {len(ck)} - {list(ck)[:5]}..\")\n",
    " \n",
    "        # count occurrences of candidates\n",
    "        for transaction in tqdm(transactions):\n",
    "            transaction_set = frozenset(transaction)\n",
    "            for candidate in ck:\n",
    "                if candidate.issubset(transaction_set):\n",
    "                    ck[candidate] += 1\n",
    "        \n",
    "        print(f\"C{k} counts: {len(ck)} - {list(ck)[:5]}..\")\n",
    "        C.append(ck)\n",
    "        \n",
    "        # filter candidates with minimum support\n",
    "        lk = {itemset: count for itemset, count in ck.items() if count >= s}\n",
    "        \n",
    "        print(f\"L{k}: {len(lk)} - {lk}\")\n",
    "        \n",
    "        # if no frequent itemsets found, break\n",
    "        if not lk:\n",
    "            break\n",
    "        L.append(lk)\n",
    "            \n",
    "    \n",
    "    return L, C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 5000.0\n",
      "C1: 870 - [frozenset({0}), frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4})]..\n",
      "L1: 10 - [frozenset({217}), frozenset({354}), frozenset({368}), frozenset({419}), frozenset({494})]..\n",
      "C2 candidates: 45 - [frozenset({217, 354}), frozenset({368, 217}), frozenset({217, 419}), frozenset({217, 494}), frozenset({217, 529})]..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 605564.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2 counts: 45 - [frozenset({217, 354}), frozenset({368, 217}), frozenset({217, 419}), frozenset({217, 494}), frozenset({217, 529})]..\n",
      "L2: 0 - {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = 0.05*len(transactions)\n",
    "print(f\"s: {s}\")\n",
    "L, C = A_priori(transactions, s, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{frozenset({217}): 5375,\n",
       "  frozenset({354}): 5835,\n",
       "  frozenset({368}): 7828,\n",
       "  frozenset({419}): 5057,\n",
       "  frozenset({494}): 5102,\n",
       "  frozenset({529}): 7057,\n",
       "  frozenset({684}): 5408,\n",
       "  frozenset({722}): 5845,\n",
       "  frozenset({766}): 6265,\n",
       "  frozenset({829}): 6810}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(frozenset({368, 829}), 1194), (frozenset({368, 494}), 860), (frozenset({368, 529}), 640), (frozenset({684, 766}), 613), (frozenset({529, 829}), 584), (frozenset({722, 354}), 566), (frozenset({368, 766}), 504), (frozenset({217, 722}), 498), (frozenset({722, 684}), 443), (frozenset({217, 529}), 403), (frozenset({368, 722}), 392), (frozenset({368, 684}), 387), (frozenset({722, 419}), 366), (frozenset({368, 419}), 355), (frozenset({684, 829}), 349), (frozenset({217, 419}), 344), (frozenset({529, 684}), 334), (frozenset({354, 766}), 329), (frozenset({722, 766}), 328), (frozenset({829, 766}), 321), (frozenset({368, 354}), 319), (frozenset({529, 766}), 317), (frozenset({368, 217}), 303), (frozenset({529, 354}), 301), (frozenset({722, 829}), 294), (frozenset({529, 722}), 283), (frozenset({217, 354}), 280), (frozenset({217, 766}), 276), (frozenset({217, 829}), 275), (frozenset({829, 494}), 267), (frozenset({354, 419}), 263), (frozenset({354, 829}), 259), (frozenset({419, 829}), 259), (frozenset({529, 419}), 252), (frozenset({419, 766}), 238), (frozenset({494, 766}), 227), (frozenset({722, 494}), 226), (frozenset({529, 494}), 225), (frozenset({354, 684}), 219), (frozenset({684, 494}), 208), (frozenset({217, 684}), 198), (frozenset({354, 494}), 189), (frozenset({217, 494}), 183), (frozenset({419, 494}), 176), (frozenset({419, 684}), 155)]\n"
     ]
    }
   ],
   "source": [
    "c2 = sorted([(v,c) for v,c in C[1].items()], key=lambda x: x[1], reverse=True)\n",
    "print(c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules\n",
    "\n",
    "1. frequent itemset I\n",
    "2. rule generation\n",
    "    - for subset A in I:\n",
    "    \n",
    "        1. conf(A -> A/I) = supp(I) / supp(A)              given A how likely to also observe A/I\n",
    "\n",
    "        2. if conf > c:\n",
    "\n",
    "            We've found a rule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: 870 - [frozenset({0}), frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4})]..\n",
      "L1: 4 - [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({5})]..\n",
      "C2 candidates: 6 - [frozenset({1, 2}), frozenset({1, 3}), frozenset({1, 5}), frozenset({2, 3}), frozenset({2, 5})]..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 106861.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2 counts: 6 - [frozenset({1, 2}), frozenset({1, 3}), frozenset({1, 5}), frozenset({2, 3}), frozenset({2, 5})]..\n",
      "L2: 4 - {frozenset({1, 2}): 3, frozenset({1, 3}): 4, frozenset({2, 3}): 5, frozenset({2, 5}): 3}\n",
      "C3 candidates: 1 - [frozenset({1, 2, 3})]..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 294337.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3 counts: 1 - [frozenset({1, 2, 3})]..\n",
      "L3: 1 - {frozenset({1, 2, 3}): 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    {1, 2, 3},    # B1 {m, c, b}\n",
    "    {1, 4, 5},    # B2 {m, p, j}\n",
    "    {1, 2, 3, 6}, # B3 {m, c, b, n}\n",
    "    {2, 5},       # B4 {c, j}\n",
    "    {1, 4, 3},    # B5 {m, p, b}\n",
    "    {1, 2, 3, 5}, # B6 {m, c, b, j}\n",
    "    {2, 3, 5},    # B7 {c, b, j}\n",
    "    {3, 2}        # B8 {b, c}\n",
    "]\n",
    "\n",
    "\n",
    "L, C = A_priori(transactions, s=3, max_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from typing import List, Dict, FrozenSet\n",
    "\n",
    "def find_association_rules(L:List[Dict[FrozenSet[int], int]], c=0.5):\n",
    "    '''\n",
    "    L: list of frequent itemsets (L1, L2, ... Lk), where Lk is a dict of itemsets and their counts\n",
    "    '''\n",
    "    \n",
    "    # get all frequent itemsets\n",
    "    frequent_itemsets = {}\n",
    "    for level in L:\n",
    "        frequent_itemsets.update(level)\n",
    "    \n",
    "    # filter out singletons\n",
    "    frequent_itemsets = list(k for k in frequent_itemsets.keys() if len(k) > 1)\n",
    "    \n",
    "    # support counts\n",
    "    counts = {}\n",
    "    for level in L:\n",
    "        counts.update(level)\n",
    "    \n",
    "    # generate rules\n",
    "    rules = {}\n",
    "    for itemset in frequent_itemsets:\n",
    "        n = len(itemset)\n",
    "        for i in range(1, n):\n",
    "            for subset in map(frozenset, itertools.combinations(itemset, i)):\n",
    "                consequent = itemset - subset\n",
    "                \n",
    "                # evaluate subset -> consequent\n",
    "                \n",
    "                if len(consequent) > 0:  # Ensure non-empty consequent\n",
    "                    conf = counts[itemset] / counts[subset]\n",
    "                    if conf >= c:\n",
    "                        rules[(subset, consequent)] = conf\n",
    "    \n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(frozenset({1}), frozenset({3})): 0.8,\n",
       " (frozenset({2}), frozenset({3})): 0.8333333333333334,\n",
       " (frozenset({3}), frozenset({2})): 0.8333333333333334,\n",
       " (frozenset({5}), frozenset({2})): 0.75,\n",
       " (frozenset({1, 2}), frozenset({3})): 1.0,\n",
       " (frozenset({1, 3}), frozenset({2})): 0.75}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_association_rules(L, c=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "1. Find frequent itemsets\n",
    "2. Find association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 1000.0\n",
      "C1: 870 - [frozenset({0}), frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4})]..\n",
      "L1: 375 - [frozenset({1}), frozenset({4}), frozenset({5}), frozenset({6}), frozenset({8})]..\n",
      "C2 candidates: 70125 - [frozenset({1, 4}), frozenset({1, 5}), frozenset({1, 6}), frozenset({8, 1}), frozenset({1, 10})]..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:13<00:00, 516.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2 counts: 70125 - [frozenset({1, 4}), frozenset({1, 5}), frozenset({1, 6}), frozenset({8, 1}), frozenset({1, 10})]..\n",
      "L2: 9 - {frozenset({704, 39}): 1107, frozenset({825, 39}): 1187, frozenset({217, 346}): 1336, frozenset({227, 390}): 1049, frozenset({368, 682}): 1193, frozenset({368, 829}): 1194, frozenset({722, 390}): 1042, frozenset({704, 825}): 1102, frozenset({829, 789}): 1194}\n",
      "C3 candidates: 1 - [frozenset({704, 825, 39})]..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 1997401.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3 counts: 1 - [frozenset({704, 825, 39})]..\n",
      "L3: 1 - {frozenset({704, 825, 39}): 1035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transactions = load_transactions('data/T10I4D100K.dat')\n",
    "\n",
    "s = 0.01*len(transactions)\n",
    "print(f\"s: {s}\")\n",
    "L, C = A_priori(transactions, s, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(frozenset({704}), frozenset({39})): 0.617056856187291,\n",
       " (frozenset({227}), frozenset({390})): 0.577007700770077,\n",
       " (frozenset({704}), frozenset({825})): 0.6142697881828316,\n",
       " (frozenset({704}), frozenset({39, 825})): 0.5769230769230769,\n",
       " (frozenset({704, 825}), frozenset({39})): 0.9392014519056261,\n",
       " (frozenset({39, 704}), frozenset({825})): 0.9349593495934959,\n",
       " (frozenset({39, 825}), frozenset({704})): 0.8719460825610783}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_association_rules(L, c=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The association rules suggests that if 704 and 825 is bought, then 39 is also bought, with a high condifence of almost 94%. There seem to be a strong correlation between all of these three products, while it seems like 39 and 825 is more often bought without 704 than the others. \n",
    "\n",
    "if 704 is bought, then you can be quite confident of the customer also buying 30 or 825 ~60%\n",
    "if 704 is bought with either 39 or 825, then with a confidence of more than 93% we can say that they'll also buy all three. \n",
    "\n",
    "\n",
    "It would be clever to arrange all of these products close to each other in the store.\n",
    "\n",
    "226 and 390 also seems to have some correlation, yet lower at conf~58%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
