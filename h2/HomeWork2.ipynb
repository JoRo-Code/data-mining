{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee35bbd8-3d35-4caf-922b-c03b65c289c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50003718-3e50-487e-9246-a0cc537a57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b939a1b-9ea4-49b9-9f3a-4a9b1f2119d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_generate_transactionslist():\n",
    "    tr_file = \"T10I4D100K.dat\"\n",
    "    \n",
    "    #Read transactions file and convert the format to a list of transaction-lists\n",
    "    # the transaction values are integers\n",
    "    with open(tr_file, 'r') as file:\n",
    "        data_set = [list(map(int, line.split())) for line in file]\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eac98ff1-5a07-4a24-85f2-a9bc0df274f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct(k, Lk_minus, data_set):\n",
    "    Ck = set()\n",
    "    if k == 1:\n",
    "        min_val = min(value for transaction in data_set for value in transaction)\n",
    "        max_val = max(value for transaction in data_set for value in transaction)\n",
    "        unique_values = [[uv] for uv in range(min_val,max_val+1)]\n",
    "        Ck = unique_values\n",
    "    else:\n",
    "        \"\"\"\n",
    "        for i in range(len(Lk_minus)):\n",
    "            itemset_i = Lk_minus[i]\n",
    "            if i+1 < len(Lk_minus):\n",
    "                #remaining_sublist = Lk_minus[i+1:]\n",
    "                for itemset_j in Lk_minus[i+1:]:\n",
    "                    for value in itemset_j:\n",
    "                        if value not in itemset_i:\n",
    "                            candidate_i = list(itemset_i.copy())\n",
    "                            candidate_i.append(value)\n",
    "                            Ck.add(tuple(candidate_i))\n",
    "        \"\"\"\n",
    "        #picks all possible combinations of two itemsets; itemset_i, itemset_j\n",
    "        for itemset_i, itemset_j in itertools.combinations(Lk_minus, 2):\n",
    "            union_i = set(itemset_i).union(itemset_j)\n",
    "            # generate all possible combinations of length k from the union of the old itemsets\n",
    "            for candidate_i in itertools.combinations(union_i, k):  \n",
    "                #add to a set in order to prevent duplicate candidates\n",
    "                Ck.add(tuple(candidate_i))  \n",
    "        Ck = list(Ck)\n",
    "                \n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f93bdacc-6dda-4948-b75c-3af8a70bf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(itemset_i, data_set):\n",
    "    itemset_i_set = set(itemset_i)  \n",
    "    occurences = 0\n",
    "    for transaction in data_set:\n",
    "        transaction_set = set(transaction)  \n",
    "        if itemset_i_set.issubset(transaction_set): \n",
    "            occurences += 1\n",
    "    support = occurences / len(data_set)\n",
    "    return support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11757147-cf16-4c16-a68c-7314e0cb5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(s, Ck, data_set):\n",
    "    \"\"\"\n",
    "    Lk = []\n",
    "    for itemset_i in Ck:\n",
    "        occurences = 0\n",
    "        for transaction in data_set:\n",
    "            occurences += np.sum(np.all(np.isin(np.array(list(itemset_i)), np.array(transaction))))\n",
    "        support = occurences/len(data_set)\n",
    "        if support>=s:\n",
    "            Lk.append(list(itemset_i))\n",
    "    \"\"\"\n",
    "    Lk = []\n",
    "    for itemset_i in Ck:\n",
    "        support = calculate_support(itemset_i, data_set)\n",
    "        if support >= s:\n",
    "            Lk.append(list(itemset_i))\n",
    "    return Lk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a2412e2-9624-45cb-b316-44f8434f774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(k, data_set):\n",
    "    #support s, threshold of minimum transactions containing the itemset:\n",
    "    s = 0.01\n",
    "    #max tr length:\n",
    "    #tl = len(max(data_set, key=len))\n",
    "    # use k = 3\n",
    "    Lk_minus = None\n",
    "    for k in range(1,k+1):\n",
    "        start_constr_k = time.time()\n",
    "        Ck = construct(k, Lk_minus, data_set)\n",
    "        end_constr_k = time.time()\n",
    "        print(f\"elapsed time to construct C{k}: {end_constr_k-start_constr_k} seconds\")\n",
    "        start_filter_k = time.time()\n",
    "        Lk = filter(s, Ck, data_set)\n",
    "        end_filter_k = time.time()\n",
    "        print(f\"elapsed time to filter C{k} into L{k}: {end_filter_k-start_filter_k} seconds\")\n",
    "        Lk_minus = Lk\n",
    "    freq_itemsets = Lk\n",
    "    return freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "667b4f86-82a9-45f4-be53-d9ae545a492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association_rules(k, freq_itemsets, data_set):\n",
    "    association_rules = \"supported_association_rules.csv\"\n",
    "    # confidence, threshold for association rules\n",
    "    c = 0.50\n",
    "    #dont think this is needed, since already calculated:\n",
    "    #s = 0.01\n",
    "    # Create and write the headers to the CSV file\n",
    "    with open(association_rules, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['X', 'Y', 'Support', 'Confidence'])\n",
    "        \n",
    "        # go through each itemset in freq_itemsets:\n",
    "        for freq_set in freq_itemsets:\n",
    "            # split them in each possible combination X, Y, such that S=X⋃Y :\n",
    "            #generates all possible combinations of length k-1 of the current frequent itemset\n",
    "            for X in itertools.combinations(freq_set, k-1):\n",
    "                Y = set(freq_set) - set(X)\n",
    "        \n",
    "                # calculate the support of the association rule X->Y, i.e. the number of transactions that contain X⋃Y=S, I use fraction\n",
    "                #S = freq_set\n",
    "                support_S = calculate_support(freq_set, data_set)\n",
    "            \n",
    "                # calculate the confidence of the association rule X->Y, i.e. the fraction of transactions containing X⋃Y \n",
    "                # in all transactions that contain X. support(S)/support(X)\n",
    "                support_X = calculate_support(X, data_set)\n",
    "                confidence_i = support_S/support_X\n",
    "                # if the association rule passes the thresholds of c and s, add the sheet of supported association rules:\n",
    "                if confidence_i>= c:\n",
    "                    writer.writerow([X, Y, support_S, confidence_i])\n",
    "    \n",
    "    df = pd.read_csv(association_rules)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de4724-25cd-4cca-a6fa-641a4f048838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time to construct C1: 0.1537792682647705 seconds\n",
      "elapsed time to filter C1 into L1: 89.65252733230591 seconds\n",
      "elapsed time to construct C2: 0.08660888671875 seconds\n"
     ]
    }
   ],
   "source": [
    "#read transactions file and generate it to a list of transactions-lists:\n",
    "data_set = read_and_generate_transactionslist()\n",
    "#print(len(data_set)) #100,000\n",
    "\n",
    "k = 3\n",
    "start_apriori = time.time()\n",
    "# generate the list of frequent itemsets (a list of lists):\n",
    "freq_itemsets = apriori(k, data_set)\n",
    "end_apriori = time.time()\n",
    "print(f\"total time to obtain the frequent itemsets using the apriori algorithm: {end_apriori-start_apriori} seconds\")\n",
    "#print(freq_itemsets)\n",
    "\n",
    "# evaluate association rules that are supported by the support and confidense criterias\n",
    "# and visualize their representation in a csv-format:\n",
    "start_association_rules = time.time()\n",
    "generate_association_rules(k, freq_itemsets, data_set)\n",
    "end_association_rules = time.time()\n",
    "print(f\"total time to evaluate association rules: {end_association_rules-start_association_rules} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de8ff7-43ec-41b1-b047-d4208ce663fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
