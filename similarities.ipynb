{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar documents\n",
    "\n",
    "This notebook aims to use a pipeline to find similar documents.\n",
    "\n",
    "The pipeline is as follows:\n",
    "\n",
    "graph LR\n",
    "\n",
    "    A[Documents] --> B[Tokenization]\n",
    "    B --> C[Shingling]\n",
    "    C --> D[MinHashing]\n",
    "    D --> E[LSH]\n",
    "    E --> F[Similarity Check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools \n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "First step that we need to do is to find data to check for similarity.\n",
    "We found a dataset with 10 different types of documents with 100 docs each.\n",
    "This was downloaded and put into the current working directory in a folder called `data`\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jensenbaxter/10dataset-text-document-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "After downloading the data, the documents have to be read from memory everytime we want to process them\n",
    "\n",
    "```python\n",
    "docs = read_documents(\"data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    documents.append(f.read())\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This step involves converting the docs into tokens. \n",
    "\n",
    "E.g. \"A cat eats\" -> [0,1,2]\n",
    "\n",
    "Where \n",
    "vocab = {\n",
    "    0: a, \n",
    "    1: cat,\n",
    "    2, eats\n",
    "}\n",
    "\n",
    "We can use a more complicated tokenizer, e.g. BPE. This would speed up processing larger quantities of data since a number would represent a larger chunk. \n",
    "\n",
    "Nevertheless,\n",
    "using a word level tokenizer is fine for the purpose of this exercise. Our main goal is to convert the documents into integer representations, allowing for efficient and more general computations. E.g. by only using numbers we can restrict memory usage with uint16 etc which allows for quicker computes. \n",
    "\n",
    "In this tokenizer, we are compressing the vocab slightly more by ignoring capitalizations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        for doc in documents:\n",
    "            doc = doc.lower()\n",
    "            for word in doc.split():\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = len(self.vocabulary)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.vocabulary[word.lower()] for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.vocabulary[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling\n",
    "\n",
    "shingle size 2\n",
    "\n",
    "abcba -> [ab, bc, cb, ab]\n",
    "\n",
    "unique set -> {ab, bc, cb}\n",
    "\n",
    "\n",
    "TODO:\n",
    "To further optimize the shingling process, we should use less overlap.\n",
    "\n",
    "shingle size 3, with 1 overlap\n",
    "\n",
    "abcdefghij -> [abc, cde, efg, ghi, ij]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the hash is deterministic\n",
    "# use ordered set\n",
    "# overlap \n",
    "class Shingling:\n",
    "    def __init__(self):\n",
    "        self.shingles = set()\n",
    "        self.hashed_shingles = set()\n",
    "    \n",
    "    def create_shingles(self, text, k):\n",
    "        # Create k-shingles as tuples\n",
    "        self.shingles = {tuple(text[i:i+k]) for i in range(len(text) - k + 1)}\n",
    "        \n",
    "        # Hash each shingle and store in sorted order\n",
    "        self.hashed_shingles = {hash(shingle) for shingle in self.shingles}\n",
    "        \n",
    "        return self.hashed_shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-3165226637586315787, 529344067295497451, 4003026094496801395}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Shingling()\n",
    "s.create_shingles([1,2,3,4,5], 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing\n",
    "\n",
    "This step can be done in a lot of different ways, the main idea is to compress doc shingle representations.\n",
    "\n",
    "\n",
    "\n",
    "Below two different methods are shown. \n",
    "1. using permutations to simulate hashfunctions\n",
    "    The permutations are used as masks for the original representation. Then, from the masked representation we take the min value, and use that as the compressed information for that particular permutation. This is done for n steps, to create a compressed n vector representation of the single\n",
    "2. using hashfunctions \n",
    "    generating n different hashfunctions\n",
    "        generating random params to universal hashing formula: \n",
    "            fixed seed. Its not necessary since we're keeping the same for the entire sig matrix generation.\n",
    "        \n",
    "            `((ax + b) % p) % m`\n",
    "\n",
    "            x: is the integer representation of each shingle\n",
    "\n",
    "            a, b is generated\n",
    "            \n",
    "            m is the universal size\n",
    "\n",
    "            p is the next prime bigger than the universal size (number of unique shingles), to bin each shingle to its own\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class MinHashing:\n",
    "    def __init__(self):\n",
    "        self.unique_shingles = set()\n",
    "        \n",
    "    def fit(self, sets:List[set]):\n",
    "        for set_ in sets:\n",
    "            self.unique_shingles.update(set_)\n",
    "\n",
    "    # def characteristic_vector(self, shingles: set):\n",
    "    #     shingles_array = np.array(list(self.unique_shingles))\n",
    "    #     return np.isin(shingles_array, list(shingles)).astype(np.int8)\n",
    "    \n",
    "    # def signature(self, n, shingles:set):\n",
    "    #     np.random.seed(n)\n",
    "        \n",
    "    #     permutations = np.array([np.random.permutation(len(self.unique_shingles)) for _ in range(n)])\n",
    "    #     characteristic_vector = self.characteristic_vector(shingles)\n",
    "    #     permuted_shingles = permutations * characteristic_vector\n",
    "    #     minhash = np.array([np.min(row[row != 0]) if np.any(row != 0) else 0 for row in permuted_shingles])\n",
    "    #     return minhash\n",
    "\n",
    "    def _hash_function(self, x: int, a: int, b: int, p: int, m: int) -> int:\n",
    "        return ((a * x + b) % p) % m\n",
    "    \n",
    "        \n",
    "    def signature_matrix(self, n: int, shingles: List[set]):\n",
    "        self.fit(shingles)\n",
    "        m = len(self.unique_shingles)\n",
    "        p = next_prime(m)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # generate n different hashfunctions\n",
    "        hash_params = [(np.random.randint(1, p), np.random.randint(0, p)) for _ in range(n)]\n",
    "        \n",
    "        signature_matrix = np.full((len(shingles), n), np.inf)\n",
    "        \n",
    "        shingle_to_int = {shingle: idx for idx, shingle in enumerate(self.unique_shingles)}\n",
    "        \n",
    "        \n",
    "        # create signature for each document\n",
    "        for i, doc_shingles in enumerate(tqdm(shingles, desc=\"MinHashing\")):\n",
    "            \n",
    "            for shingle in doc_shingles:\n",
    "                x = shingle_to_int[shingle]\n",
    "                \n",
    "                # apply each hashfunction to get the signature\n",
    "                for j, (a, b) in enumerate(hash_params):\n",
    "                    hash_value = self._hash_function(x, a, b, p, m)\n",
    "                    signature_matrix[i, j] = min(signature_matrix[i, j], hash_value)\n",
    "        \n",
    "        return signature_matrix.astype(np.int32)\n",
    "\n",
    "def next_prime(n):\n",
    "    def is_prime(num):\n",
    "        if num < 2:\n",
    "            return False\n",
    "        for i in range(2, int(num ** 0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    next_num = n + 1\n",
    "    while not is_prime(next_num):\n",
    "        next_num += 1\n",
    "    return next_num\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH\n",
    "\n",
    "Locality-sensitive hashing basically mean that we just check a small portion of the signatures and if that portion is the same for two signatures, we consider them to be similar documents. The larger bands we have, more stuff needs to be the same for it to be a match, making us pickier. \n",
    "\n",
    "We hash every band for each signature\n",
    "\n",
    "If there's a signature band collision, two signatures being hashed to the same bucket, we consider them being similar documents\n",
    "\n",
    "To generate all candidate pairs we give all the combinations of collisions in the buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def candidate_pairs(self, signatures:np.array, b, r):\n",
    "        unique_pairs = set()\n",
    "        \n",
    "        # Add progress bar for band processing\n",
    "        for i in tqdm(range(b), desc=\"Processing bands\"):\n",
    "            band = signatures[:, i*r:(i+1)*r]\n",
    "            \n",
    "            # Create buckets using band hash\n",
    "            buckets = {}\n",
    "            for doc_idx, band_signature in enumerate(band):\n",
    "                # Convert band signature to bytes for hashing\n",
    "                band_hash = hash(band_signature.tobytes())\n",
    "                if band_hash not in buckets:\n",
    "                    buckets[band_hash] = []\n",
    "                buckets[band_hash].append(doc_idx)\n",
    "            \n",
    "            # Only compare documents that hash to the same bucket\n",
    "            for bucket in buckets.values():\n",
    "                if len(bucket) > 1:  # Only process buckets with collisions\n",
    "                    for j, k in itertools.combinations(bucket, 2):\n",
    "                        unique_pairs.add((j, k))\n",
    "        \n",
    "        return unique_pairs \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similiarity Check\n",
    "\n",
    "The similarity check used here is the jaccard similarity. The intersection of two sets divided by the union.\n",
    "Depending on the representations of the sets, this will be calculated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSets:\n",
    "    def jaccard(self, a, b):\n",
    "        return len(a.intersection(b)) / len(a.union(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CompareSignatures:\n",
    "    def jaccard(self, a:np.array, b:np.array):\n",
    "        return np.mean(a == b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline...\n",
      "✓ Reading documents: 0.07s\n",
      "✓ Tokenization: 0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shingling: 100%|██████████| 1000/1000 [00:00<00:00, 5015.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Shingling: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHashing: 100%|██████████| 1000/1000 [00:06<00:00, 156.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MinHashing: 6.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing bands: 100%|██████████| 10/10 [00:00<00:00, 270.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LSH: 0.04s\n",
      "\n",
      "Total pipeline time: 6.85s\n",
      "\n",
      "Similarity Results:\n",
      "Number of candidate pairs: 20\n",
      "\n",
      "Candidate pair: 527, 574\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 0.993103448275862\n",
      "\n",
      "Candidate pair: 432, 486\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 111, 199\n",
      "MinHash Jaccard similarity: 0.72\n",
      "Shingle Jaccard similarity: 0.7830188679245284\n",
      "\n",
      "Candidate pair: 277, 298\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 133, 162\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 724, 798\n",
      "MinHash Jaccard similarity: 0.7\n",
      "Shingle Jaccard similarity: 0.6666666666666666\n",
      "\n",
      "Candidate pair: 362, 366\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 440, 465\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 464, 490\n",
      "MinHash Jaccard similarity: 0.94\n",
      "Shingle Jaccard similarity: 0.9430379746835443\n",
      "\n",
      "Candidate pair: 318, 387\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 60, 76\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 0.9817518248175182\n",
      "\n",
      "Candidate pair: 419, 469\n",
      "MinHash Jaccard similarity: 0.76\n",
      "Shingle Jaccard similarity: 0.8\n",
      "\n",
      "Candidate pair: 322, 329\n",
      "MinHash Jaccard similarity: 0.98\n",
      "Shingle Jaccard similarity: 0.9802197802197802\n",
      "\n",
      "Candidate pair: 54, 84\n",
      "MinHash Jaccard similarity: 0.98\n",
      "Shingle Jaccard similarity: 0.9719827586206896\n",
      "\n",
      "Candidate pair: 125, 144\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 520, 530\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 0.9942196531791907\n",
      "\n",
      "Candidate pair: 851, 861\n",
      "MinHash Jaccard similarity: 0.54\n",
      "Shingle Jaccard similarity: 0.5806451612903226\n",
      "\n",
      "Candidate pair: 36, 38\n",
      "MinHash Jaccard similarity: 1.0\n",
      "Shingle Jaccard similarity: 1.0\n",
      "\n",
      "Candidate pair: 560, 593\n",
      "MinHash Jaccard similarity: 0.9\n",
      "Shingle Jaccard similarity: 0.8478260869565217\n",
      "\n",
      "Candidate pair: 105, 165\n",
      "MinHash Jaccard similarity: 0.92\n",
      "Shingle Jaccard similarity: 0.9351851851851852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N = 50\n",
    "B = 10\n",
    "R = 5\n",
    "K = 7\n",
    "\n",
    "# Read documents\n",
    "print(\"Starting pipeline...\")\n",
    "start = time.time()\n",
    "docs = read_documents(\"data\")\n",
    "read_time = time.time() - start\n",
    "print(f\"✓ Reading documents: {read_time:.2f}s\")\n",
    "\n",
    "# Tokenization\n",
    "start = time.time()\n",
    "t = Tokenizer()\n",
    "t.fit(docs)\n",
    "tokenization_time = time.time() - start\n",
    "print(f\"✓ Tokenization: {tokenization_time:.2f}s\")\n",
    "\n",
    "# Shingling\n",
    "start = time.time()\n",
    "s = Shingling()\n",
    "shingled_docs = [s.create_shingles(t.encode(doc), K) for doc in tqdm(docs, desc=\"Shingling\")]\n",
    "shingling_time = time.time() - start\n",
    "print(f\"✓ Shingling: {shingling_time:.2f}s\")\n",
    "\n",
    "# MinHashing\n",
    "start = time.time()\n",
    "mh = MinHashing()\n",
    "sig_matrix = mh.signature_matrix(N, shingled_docs)\n",
    "minhashing_time = time.time() - start\n",
    "print(f\"✓ MinHashing: {minhashing_time:.2f}s\")\n",
    "\n",
    "# LSH\n",
    "start = time.time()\n",
    "c = CompareSignatures()\n",
    "candidate_pairs = list(LSH().candidate_pairs(sig_matrix, b=B, r=R))\n",
    "lsh_time = time.time() - start\n",
    "print(f\"✓ LSH: {lsh_time:.2f}s\")\n",
    "\n",
    "total_time = read_time + tokenization_time + shingling_time + minhashing_time + lsh_time\n",
    "print(f\"\\nTotal pipeline time: {total_time:.2f}s\")\n",
    "\n",
    "# Print similarity results\n",
    "print(\"\\nSimilarity Results:\")\n",
    "print(f\"Number of candidate pairs: {len(candidate_pairs)}\")\n",
    "for x, y in candidate_pairs:\n",
    "    print(f\"\\nCandidate pair: {x}, {y}\")\n",
    "    print(f\"MinHash Jaccard similarity: {c.jaccard(sig_matrix[x], sig_matrix[y])}\")\n",
    "    print(f\"Shingle Jaccard similarity: {CompareSets().jaccard(shingled_docs[x], shingled_docs[y])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_documents(docs, x, y, max_width=70):\n",
    "    def wrap_text(text, width):\n",
    "        # Split text into chunks of max_width characters\n",
    "        return [text[i:i+width] for i in range(0, len(text), width)]\n",
    "    \n",
    "    doc1_lines = docs[x].split(\"\\n\")\n",
    "    doc2_lines = docs[y].split(\"\\n\")\n",
    "    \n",
    "    # Wrap long lines\n",
    "    doc1_wrapped = [line for text in doc1_lines for line in wrap_text(text, max_width)]\n",
    "    doc2_wrapped = [line for text in doc2_lines for line in wrap_text(text, max_width)]\n",
    "    \n",
    "    print(f\"\\n{'='*100}\\nComparing documents {x} and {y}\\n{'='*100}\")\n",
    "    print(f\"{'Document ' + str(x):<{max_width+5}} | {'Document ' + str(y)}\")\n",
    "    print(f\"{'-'*(max_width+5)}-+-{'-'*max_width}\")\n",
    "    \n",
    "    for line1, line2 in itertools.zip_longest(doc1_wrapped, doc2_wrapped, fillvalue=\"\"):\n",
    "        print(f\"{line1:<{max_width+5}} | {line2}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets investigate the minhash similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((36, 38), np.float64(1.0)),\n",
       " ((60, 76), np.float64(1.0)),\n",
       " ((125, 144), np.float64(1.0)),\n",
       " ((133, 162), np.float64(1.0)),\n",
       " ((277, 298), np.float64(1.0)),\n",
       " ((318, 387), np.float64(1.0)),\n",
       " ((362, 366), np.float64(1.0)),\n",
       " ((432, 486), np.float64(1.0)),\n",
       " ((440, 465), np.float64(1.0)),\n",
       " ((520, 530), np.float64(1.0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the pair wise signature similarities\n",
    "pairs = list(itertools.combinations(range(len(sig_matrix)), 2))\n",
    "pair_similarities = [((x,y), c.jaccard(sig_matrix[x], sig_matrix[y])) for x, y in pairs]\n",
    "\n",
    "pair_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pair_similarities[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1605484112.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Similarity: {sim:.4f}\"i)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "for (x, y), sim in pair_similarities:\n",
    "    print(f\"Similarity: {sim:.4f}\")\n",
    "    view_documents(docs, x, y, max_width=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
