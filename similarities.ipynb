{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar documents\n",
    "\n",
    "This notebook aims to use a pipeline to find similar documents.\n",
    "\n",
    "The pipeline is as follows:\n",
    "\n",
    "graph LR\n",
    "\n",
    "    A[Documents] --> B[Tokenization]\n",
    "    B --> C[Shingling]\n",
    "    C --> D[MinHashing]\n",
    "    D --> E[LSH]\n",
    "    E --> F[Similarity Check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools \n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "First step that we need to do is to find data to check for similarity.\n",
    "We found a dataset with 10 different types of documents with 100 docs each.\n",
    "This was downloaded and put into the current working directory in a folder called `data`\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jensenbaxter/10dataset-text-document-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "After downloading the data, the documents have to be read from memory everytime we want to process them\n",
    "\n",
    "```python\n",
    "docs = read_documents(\"data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    documents.append(f.read())\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This step involves converting the docs into tokens. \n",
    "\n",
    "E.g. \"A cat eats\" -> [0,1,2]\n",
    "\n",
    "Where \n",
    "vocab = {\n",
    "    0: a, \n",
    "    1: cat,\n",
    "    2: eats,\n",
    "}\n",
    "\n",
    "We can use a more complicated tokenizer, e.g. BPE. This would speed up processing larger quantities of data since a number would represent a larger chunk. \n",
    "\n",
    "Nevertheless,\n",
    "using a word level tokenizer is fine for the purpose of this exercise. Our main goal is to convert the documents into integer representations, allowing for efficient and more general computations. E.g. by only using numbers we can restrict memory usage with uint16 etc which allows for quicker computes. \n",
    "\n",
    "In this tokenizer, we are compressing the vocab slightly more by ignoring capitalizations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        for doc in documents:\n",
    "            doc = doc.lower()\n",
    "            for word in doc.split():\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = len(self.vocabulary)\n",
    "        return self\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.vocabulary[word.lower()] for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join([self.vocabulary[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling\n",
    "\n",
    "shingle size 2\n",
    "\n",
    "abcba -> [ab, bc, cb, ab]\n",
    "\n",
    "unique set -> {ab, bc, cb}\n",
    "\n",
    "\n",
    "TODO:\n",
    "To further optimize the shingling process, we should use less overlap.\n",
    "\n",
    "shingle size 3, with 1 overlap\n",
    "\n",
    "abcdefghij -> [abc, cde, efg, ghi, ij]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Shingling:\n",
    "    @staticmethod\n",
    "    def create_shingles(text, k):\n",
    "        # Create k-shingles as tuples\n",
    "        shingles = {tuple(text[i:i+k]) for i in range(len(text) - k + 1)}\n",
    "        \n",
    "        # Hash each shingle and store in sorted order\n",
    "        hashed_shingles = {hash(shingle) for shingle in shingles}\n",
    "        \n",
    "        return hashed_shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-3165226637586315787, 529344067295497451, 4003026094496801395}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shingling.create_shingles([1,2,3,4,5], 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing\n",
    "\n",
    "This step can be done in a lot of different ways, the main idea is to compress doc shingle representations.\n",
    "\n",
    "\n",
    "\n",
    "Below two different methods are shown. \n",
    "1. using permutations to simulate hashfunctions\n",
    "    The permutations are used as masks for the original representation. Then, from the masked representation we take the min value, and use that as the compressed information for that particular permutation. This is done for n steps, to create a compressed n vector representation of the single\n",
    "2. using hashfunctions \n",
    "    generating n different hashfunctions\n",
    "        generating random params to universal hashing formula: \n",
    "            fixed seed. Its not necessary since we're keeping the same for the entire sig matrix generation.\n",
    "        \n",
    "            `((ax + b) % p) % m`\n",
    "\n",
    "            x: is the integer representation of each input (this case shingles)\n",
    "\n",
    "            a: randomly generated. needs to be in the range [1, p-1]. Can't be zero since then the function would be simplified to `(b % p) % m`, hashing some inputs to the same values, which is undesirable. Should be less than p, since otherwise some inputs could get hashed to the same values. \n",
    "\n",
    "            b: randomly generated.\n",
    "            same as 'a' but can be zero since the input is not removed. Would be simplified to (ax % p) % m\n",
    "            \n",
    "            m: the universal size\n",
    "            It would be fine to not use `% m`. Doing so will ensure the output is in the desired range M instead of P.\n",
    "\n",
    "            p: prime number\n",
    "            the next prime bigger than the universal size (number of unique shingles), to bin each shingle to its own. It ensures good distribution properties. \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class MinHashing:\n",
    "    def __init__(self):\n",
    "        self.unique_shingles = set()\n",
    "        \n",
    "    def fit(self, sets:List[set]):\n",
    "        for set_ in sets:\n",
    "            self.unique_shingles.update(set_)\n",
    "\n",
    "    # def characteristic_vector(self, shingles: set):\n",
    "    #     shingles_array = np.array(list(self.unique_shingles))\n",
    "    #     return np.isin(shingles_array, list(shingles)).astype(np.int8)\n",
    "    \n",
    "    # def signature(self, n, shingles:set):\n",
    "    #     np.random.seed(n)\n",
    "        \n",
    "    #     permutations = np.array([np.random.permutation(len(self.unique_shingles)) for _ in range(n)])\n",
    "    #     characteristic_vector = self.characteristic_vector(shingles)\n",
    "    #     permuted_shingles = permutations * characteristic_vector\n",
    "    #     minhash = np.array([np.min(row[row != 0]) if np.any(row != 0) else 0 for row in permuted_shingles])\n",
    "    #     return minhash\n",
    "\n",
    "    def _hash_function(self, x: int, a: int, b: int, p: int, m: int) -> int:\n",
    "        return ((a * x + b) % p) % m\n",
    "        \n",
    "    def signature_matrix(self, n: int, shingles: List[set]):\n",
    "        self.fit(shingles)\n",
    "        m = len(self.unique_shingles)\n",
    "        p = next_prime(m)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # generate n different hashfunctions\n",
    "        hash_params = [(np.random.randint(1, p), np.random.randint(0, p)) for _ in range(n)]\n",
    "        \n",
    "        signature_matrix = np.full((len(shingles), n), np.inf)\n",
    "        \n",
    "        # universal index\n",
    "        shingle_to_int = {shingle: idx for idx, shingle in enumerate(self.unique_shingles)}\n",
    "        \n",
    "        # create signature for each document\n",
    "        for i, doc_shingles in enumerate(tqdm(shingles, desc=\"MinHashing\")):\n",
    "            \n",
    "            for shingle in doc_shingles:\n",
    "                x = shingle_to_int[shingle]\n",
    "                \n",
    "                # apply each hashfunction to get the signature\n",
    "                for j, (a, b) in enumerate(hash_params):\n",
    "                    hash_value = self._hash_function(x, a, b, p, m)\n",
    "                    signature_matrix[i, j] = min(signature_matrix[i, j], hash_value)\n",
    "        \n",
    "        return signature_matrix.astype(np.int32)\n",
    "\n",
    "def next_prime(n):\n",
    "    def is_prime(num):\n",
    "        if num < 2:\n",
    "            return False\n",
    "        for i in range(2, int(num ** 0.5) + 1):\n",
    "            if num % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    next_num = n + 1\n",
    "    while not is_prime(next_num):\n",
    "        next_num += 1\n",
    "    return next_num\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH\n",
    "\n",
    "Locality-sensitive hashing basically mean that we just check a small portion of the signatures and if that portion is the same for two signatures, we consider them to be similar documents. The larger bands we have, more stuff needs to be the same for it to be a match, making us pickier. \n",
    "\n",
    "We hash every band for each signature\n",
    "\n",
    "If there's a signature band collision, two signatures being hashed to the same bucket, we consider them being similar documents\n",
    "\n",
    "To generate all candidate pairs we give all the combinations of collisions in the buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    @staticmethod\n",
    "    def candidate_pairs(signatures:np.array, b, r):\n",
    "        \n",
    "        assert signatures.shape[1] % b == 0, \"The number of columns in the signature matrix must be divisible by the number of bands\"\n",
    "        unique_pairs = set()\n",
    "        \n",
    "        band_buckets = [{} for _ in range(b)]\n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(b), desc=\"Processing bands\"):\n",
    "            band = signatures[:, i*r:(i+1)*r]\n",
    "            \n",
    "            # Create buckets using band hash\n",
    "            band_bucket = band_buckets[i]\n",
    "            for doc_idx, band_signature in enumerate(band):\n",
    "                # Convert band signature to bytes for hashing\n",
    "                band_hash = hash(band_signature.tobytes())\n",
    "                if band_hash not in band_bucket:\n",
    "                    band_bucket[band_hash] = []\n",
    "                band_bucket[band_hash].append(doc_idx)\n",
    "            \n",
    "            # Only compare documents that hash to the same bucket\n",
    "            for bucket in band_bucket.values():\n",
    "                if len(bucket) > 1:  # Only process buckets with collisions\n",
    "                    for j, k in itertools.combinations(bucket, 2):\n",
    "                        unique_pairs.add((j, k))\n",
    "        \n",
    "        return unique_pairs \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similiarity Check\n",
    "\n",
    "The similarity check used here is the jaccard similarity. The intersection of two sets divided by the union.\n",
    "Depending on the representations of the sets, this will be calculated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSets:\n",
    "    @staticmethod\n",
    "    def jaccard(a:set, b:set):\n",
    "        return len(a.intersection(b)) / len(a.union(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompareSignatures:\n",
    "    @staticmethod\n",
    "    def jaccard(a:np.array, b:np.array):\n",
    "        return np.mean(a == b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Below we are using all the previously explained methods to find similar documents\n",
    "\n",
    "The parameters N, B, R can be chosen for a more or less granular search\n",
    "\n",
    "K tells how many tokens are used per shingle. A higher K would mean that it represents a more unique part of the document, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting similarity search...\n",
      "✓ Reading documents: 0.14s\n",
      "✓ Tokenization: 0.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shingling: 100%|██████████| 1000/1000 [00:00<00:00, 4927.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Shingling: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHashing: 100%|██████████| 1000/1000 [00:06<00:00, 151.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MinHashing: 6.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing bands: 100%|██████████| 10/10 [00:00<00:00, 2573.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LSH: 0.01s\n",
      "\n",
      "Total search time: 7.07s\n",
      "\n",
      "Similarity Results:\n",
      "Number of candidate pairs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Parameters\n",
    "N = 50  # Number of hashfunctions\n",
    "B = 10  # Number of bands\n",
    "R = 5   # Number of rows per band\n",
    "K = 7   # Number of tokens per shingle\n",
    "\n",
    "# Read documents\n",
    "print(\"Starting similarity search...\")\n",
    "start = time.time()\n",
    "docs = read_documents(\"data\")\n",
    "read_time = time.time() - start\n",
    "print(f\"✓ Reading documents: {read_time:.2f}s\")\n",
    "\n",
    "# Tokenization\n",
    "start = time.time()\n",
    "tokenizer = Tokenizer().fit(docs)\n",
    "tokenization_time = time.time() - start\n",
    "print(f\"✓ Tokenization: {tokenization_time:.2f}s\")\n",
    "\n",
    "# Shingling\n",
    "start = time.time()\n",
    "shingled_docs = [Shingling.create_shingles(tokenizer.encode(doc), K) for doc in tqdm(docs, desc=\"Shingling\")]\n",
    "shingling_time = time.time() - start\n",
    "print(f\"✓ Shingling: {shingling_time:.2f}s\")\n",
    "\n",
    "# MinHashing\n",
    "start = time.time()\n",
    "sig_matrix = MinHashing().signature_matrix(N, shingled_docs)\n",
    "minhashing_time = time.time() - start\n",
    "print(f\"✓ MinHashing: {minhashing_time:.2f}s\")\n",
    "\n",
    "# LSH\n",
    "start = time.time()\n",
    "candidate_pairs = list(LSH.candidate_pairs(sig_matrix, b=B, r=R))\n",
    "lsh_time = time.time() - start\n",
    "print(f\"✓ LSH: {lsh_time:.2f}s\")\n",
    "\n",
    "\n",
    "# Results\n",
    "total_time = read_time + tokenization_time + shingling_time + minhashing_time + lsh_time\n",
    "print(f\"\\nTotal search time: {total_time:.2f}s\")\n",
    "\n",
    "print(\"\\nSimilarity Results:\")\n",
    "print(f\"Number of candidate pairs: {len(candidate_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "Below we will check our found candidate pairs, the documents and their respective similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_documents(docs, x, y, max_width=70):\n",
    "    def wrap_text(text, width):\n",
    "        # Split text into chunks of max_width characters\n",
    "        return [text[i:i+width] for i in range(0, len(text), width)]\n",
    "    \n",
    "    doc1_lines = docs[x].split(\"\\n\")\n",
    "    doc2_lines = docs[y].split(\"\\n\")\n",
    "    \n",
    "    # Wrap long lines\n",
    "    doc1_wrapped = [line for text in doc1_lines for line in wrap_text(text, max_width)]\n",
    "    doc2_wrapped = [line for text in doc2_lines for line in wrap_text(text, max_width)]\n",
    "    \n",
    "    print(f\"\\n{'='*100}\\nComparing documents {x} and {y}\\n{'='*100}\")\n",
    "    print(f\"{'Document ' + str(x):<{max_width+5}} | {'Document ' + str(y)}\")\n",
    "    print(f\"{'-'*(max_width+5)}-+-{'-'*max_width}\")\n",
    "    \n",
    "    for line1, line2 in itertools.zip_longest(doc1_wrapped, doc2_wrapped, fillvalue=\"\"):\n",
    "        print(f\"{line1:<{max_width+5}} | {line2}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check out the candidate pairs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingle Similarity: 1.0000\n",
      "Signature Similarity: 1.0000\n",
      "\n",
      "====================================================================================================\n",
      "Comparing documents 40 and 65\n",
      "====================================================================================================\n",
      "Document 40                                             | Document 65\n",
      "--------------------------------------------------------+---------------------------------------------------\n",
      "Boothroyd calls for Lords speaker                       | Boothroyd calls for Lords speaker\n",
      "Betty Boothroyd has said the House of Lords needs       | Betty Boothroyd has said the House of Lords needs \n",
      "its own Speaker and that peers should lead the way      | its own Speaker and that peers should lead the way\n",
      " on reforming the upper chamber.                        |  on reforming the upper chamber.\n",
      "Baroness Boothroyd, who was the first woman to be       | Baroness Boothroyd, who was the first woman to be \n",
      "Commons Speaker, said she believed Tony Blair init      | Commons Speaker, said she believed Tony Blair init\n",
      "iated reforms without a clear outcome in mind. \"No      | iated reforms without a clear outcome in mind. \"No\n",
      "w we have to take care of it ourselves and make th      | w we have to take care of it ourselves and make th\n",
      "e best of it,\" she told the BBC's Breakfast with F      | e best of it,\" she told the BBC's Breakfast with F\n",
      "rost. In 1999 Labour removed all but 92 of the Lor      | rost. In 1999 Labour removed all but 92 of the Lor\n",
      "ds' 750 hereditary peers. That was billed as the f      | ds' 750 hereditary peers. That was billed as the f\n",
      "irst stage of reform of the institution. The lord       | irst stage of reform of the institution. The lord \n",
      "chancellor hinted further reforms could be unveile      | chancellor hinted further reforms could be unveile\n",
      "d in the next Labour manifesto.                         | d in the next Labour manifesto.\n",
      "\"I think we need to look very carefully at the rel      | \"I think we need to look very carefully at the rel\n",
      "ationship between the Lords and the Commons,\" Lord      | ationship between the Lords and the Commons,\" Lord\n",
      " Falconer told BBC1's Breakfast With Frost. \"How i      |  Falconer told BBC1's Breakfast With Frost. \"How i\n",
      "t interacts with the Commons is a very, very impor      | t interacts with the Commons is a very, very impor\n",
      "tant issue. \"We need to address the issue in the m      | tant issue. \"We need to address the issue in the m\n",
      "anifesto, but you will have to wait for when the m      | anifesto, but you will have to wait for when the m\n",
      "anifesto comes.\" The lord chancellor currently has      | anifesto comes.\" The lord chancellor currently has\n",
      " the role of House of Lords speaker. He is also he      |  the role of House of Lords speaker. He is also he\n",
      "ad of the judiciary and a member of the Cabinet as      | ad of the judiciary and a member of the Cabinet as\n",
      " constitutional affairs secretary.                      |  constitutional affairs secretary.\n",
      "Lady Boothroyd said she believed it was unacceptab      | Lady Boothroyd said she believed it was unacceptab\n",
      "le for the lord chancellor to have the role of Spe      | le for the lord chancellor to have the role of Spe\n",
      "aker. \"I would really like to see a Speaker of the      | aker. \"I would really like to see a Speaker of the\n",
      " House of Lords,\" she said. \"I don't go for the id      |  House of Lords,\" she said. \"I don't go for the id\n",
      "ea of somebody - a lord chancellor - who is head o      | ea of somebody - a lord chancellor - who is head o\n",
      "f the judiciary, a senior Cabinet minister and Spe      | f the judiciary, a senior Cabinet minister and Spe\n",
      "aker of the Lords. \"I want somebody there who is g      | aker of the Lords. \"I want somebody there who is g\n",
      "oing to look after that House and do a job there.       | oing to look after that House and do a job there.\n",
      "\n",
      "Shingle Similarity: 1.0000\n",
      "Signature Similarity: 1.0000\n",
      "\n",
      "====================================================================================================\n",
      "Comparing documents 32 and 86\n",
      "====================================================================================================\n",
      "Document 32                                             | Document 86\n",
      "--------------------------------------------------------+---------------------------------------------------\n",
      "Schools to take part in mock poll                       | Schools to take part in mock poll\n",
      "Record numbers of schools across the UK are to tak      | Record numbers of schools across the UK are to tak\n",
      "e part in a mock general election backed by the go      | e part in a mock general election backed by the go\n",
      "vernment.                                               | vernment.\n",
      "Some 600 schools have already signed up for the Y       | Some 600 schools have already signed up for the Y \n",
      "Vote Mock Elections 2005 run by the Hansard Societ      | Vote Mock Elections 2005 run by the Hansard Societ\n",
      "y and aimed at boosting interest in politics. Pupi      | y and aimed at boosting interest in politics. Pupi\n",
      "ls in the schools taking part will learn the skill      | ls in the schools taking part will learn the skill\n",
      "s of speech writers, canvassers and political cand      | s of speech writers, canvassers and political cand\n",
      "idates. Schools Minister Stephen Twigg said engagi      | idates. Schools Minister Stephen Twigg said engagi\n",
      "ng young people's interest was \"essential\" to the       | ng young people's interest was \"essential\" to the \n",
      "future of democracy.                                    | future of democracy.\n",
      "He added: said \"Young people who are engaged and m      | He added: said \"Young people who are engaged and m\n",
      "otivated by the political process are essential to      | otivated by the political process are essential to\n",
      " the future health of our democracy. \"The mock ele      |  the future health of our democracy. \"The mock ele\n",
      "ctions initiative provides an opportunity for pupi      | ctions initiative provides an opportunity for pupi\n",
      "ls to develop their own understanding of how the d      | ls to develop their own understanding of how the d\n",
      "emocratic process works and why it matters. \"By ex      | emocratic process works and why it matters. \"By ex\n",
      "periencing the election process first hand - from       | periencing the election process first hand - from \n",
      "running a campaign to the declaration of the final      | running a campaign to the declaration of the final\n",
      " result - we hope that young people will develop t      |  result - we hope that young people will develop t\n",
      "he enthusiasm to take part in the future.\" The Han      | he enthusiasm to take part in the future.\" The Han\n",
      "sard Society, the Electoral Commission and the Dep      | sard Society, the Electoral Commission and the Dep\n",
      "artment for Education and Skills are running the p      | artment for Education and Skills are running the p\n",
      "rogramme. Pupils will stand as party candidates, s      | rogramme. Pupils will stand as party candidates, s\n",
      "peech writers and canvassers. Michael Raftery, pro      | peech writers and canvassers. Michael Raftery, pro\n",
      "ject manager at the Hansard Society, said: \"The Y       | ject manager at the Hansard Society, said: \"The Y \n",
      "Vote Mock Elections for schools mirror the excitem      | Vote Mock Elections for schools mirror the excitem\n",
      "ent and buzz of a real election, raising awareness      | ent and buzz of a real election, raising awareness\n",
      " of citizenship, and the benefits of active democr      |  of citizenship, and the benefits of active democr\n",
      "acy.\" The mock votes will take place around 5 May,      | acy.\" The mock votes will take place around 5 May,\n",
      " widely expected to be the date of the general ele      |  widely expected to be the date of the general ele\n",
      "ction. Information packs, including ballot papers       | ction. Information packs, including ballot papers \n",
      "and manifesto guides, with elections happening in       | and manifesto guides, with elections happening in \n",
      "early May were sent out to the 3,000 schools invit      | early May were sent out to the 3,000 schools invit\n",
      "ed to take part.                                        | ed to take part.\n",
      "\n",
      "Shingle Similarity: 0.9430\n",
      "Signature Similarity: 0.9600\n",
      "\n",
      "====================================================================================================\n",
      "Comparing documents 64 and 90\n",
      "====================================================================================================\n",
      "Document 64                                             | Document 90\n",
      "--------------------------------------------------------+---------------------------------------------------\n",
      "Tory expert denies defeat warning                       | Tory expert denies defeatism\n",
      "The Conservatives' campaign director has denied a       | The Conservatives' campaign director has denied a \n",
      "report claiming he warned Michael Howard the party      | report claiming he warned Michael Howard the party\n",
      " could not win the next general election.               |  could not win the next general election.\n",
      "The Times on Monday said Australian Lynton Crosby       | The Times on Monday said Australian Lynton Crosby \n",
      "told the party leader to focus on trying to increa      | told the party leader to focus on trying to increa\n",
      "se the Tories' Commons presence by 25 to 30 seats.      | se the Tories' Commons presence by 25 to 30 seats.\n",
      " But Mr Crosby said in a statement: \"I have never       |  But Mr Crosby said in a statement: \"I have never \n",
      "had any such conversation... and I do not hold tha      | had any such conversation... and I do not hold tha\n",
      "t view.\" Mr Howard later added there was not \"one       | t view.\" Mr Howard later added there was not \"one \n",
      "iota\" of truth in the report. The strategist helpe      | iota\" of truth in the report. The strategist helpe\n",
      "d Australia's PM, John Howard, win four elections.      | d Australia's PM, John Howard, win four elections.\n",
      " Mr Howard appointed Mr Crosby as his elections ch      |  Mr Howard appointed Mr Crosby as his elections ch\n",
      "ief last October. Mr Crosby's statement said: \"The      | ief last October. Mr Crosby's statement said: \"The\n",
      " Conservative Party has been making an impact on t      |  Conservative Party has been making an impact on t\n",
      "he issues of lower tax and controlled immigration       | he issues of lower tax and controlled immigration \n",
      "over the past week.\" It added: \"The Labour Party w      | over the past week.\" It added: \"The Labour Party w\n",
      "ill be wanting to do all they can to distract atte      | ill be wanting to do all they can to distract atte\n",
      "ntion away from the issues that really matter to p      | ntion away from the issues that really matter to p\n",
      "eople.\"                                                 | eople.\"\n",
      "\n",
      "Shingle Similarity: 0.8000\n",
      "Signature Similarity: 0.8200\n",
      "\n",
      "====================================================================================================\n",
      "Comparing documents 19 and 69\n",
      "====================================================================================================\n",
      "Document 19                                             | Document 69\n",
      "--------------------------------------------------------+---------------------------------------------------\n",
      "Tories unveil quango blitz plans                        | Tories unveil quango blitz plans\n",
      "Plans to abolish 162 quangos have been unveiled by      | Plans to abolish 162 quangos have been unveiled by\n",
      " the Conservatives as part of their effort to show      |  the Conservatives as part of their effort to show\n",
      " how government red tape can be cut.                    |  how government red tape can be cut.\n",
      "Six government units would also be scrapped under       | Six government units would also be scrapped under \n",
      "proposals which the Tories say would save more tha      | proposals which the Tories say would save more tha\n",
      "n £4.3bn. Among the targets are strategic health a      | n £4.3bn. Among the targets are strategic health a\n",
      "uthorities and the new fair access regulator for u      | uthorities and the new fair access regulator for u\n",
      "niversities. Tory frontbencher John Redwood said B      | niversities. Tory frontbencher John Redwood said B\n",
      "ritain needed a slimmer government and lower taxes      | ritain needed a slimmer government and lower taxes\n",
      " to be competitive.                                     |  to be competitive.\n",
      "The plans would abolish regional assemblies and ot      | The plans would abolish regional assemblies and ot\n",
      "her regional bodies, such as boards tackling indus      | her regional bodies, such as boards tackling indus\n",
      "trial development and housing. Their powers would       | trial development and housing. Their powers would \n",
      "be returned to elected local councils or national       | be returned to elected local councils or national \n",
      "government. The Tories say the strategic health au      | government. The Tories say the strategic health au\n",
      "thorities are not needed as it is better that loca      | thorities are not needed as it is better that loca\n",
      "l people, rather than officials, run hospitals and      | l people, rather than officials, run hospitals and\n",
      " surgeries.                                             |  surgeries.\n",
      "Announcing the plans, Mr Redwood said: \"Mr Blair h      | Announcing the plans, Mr Redwood said: \"Mr Blair h\n",
      "as forgotten the interests of taxpayers, and has b      | as forgotten the interests of taxpayers, and has b\n",
      "roken the pledges he made. \"Far from improving pub      | roken the pledges he made. \"Far from improving pub\n",
      "lic services, spending taxpayers' money on quangos      | lic services, spending taxpayers' money on quangos\n",
      " has led only to more bureaucrats, more regulation      |  has led only to more bureaucrats, more regulation\n",
      " and higher taxes.\" His party leader, Michael Howa      |  and higher taxes.\" His party leader, Michael Howa\n",
      "rd, argued a change in direction was needed to get      | rd, argued a change in direction was needed to get\n",
      " a grip on spending. \"Labour are creating Two Brit      |  a grip on spending. \"Labour are creating Two Brit\n",
      "ains: the Britain of the forgotten majority and bu      | ains: the Britain of the forgotten majority and bu\n",
      "reaucratic Britain,\" he said. \"In the real world,       | reaucratic Britain,\" he said. \"In the real world, \n",
      "people are working harder just to stand still. The      | people are working harder just to stand still. The\n",
      "y've seen their pensions knocked for six. \"They're      | y've seen their pensions knocked for six. \"They're\n",
      " being squeezed by extra taxes. The forgotten majo      |  being squeezed by extra taxes. The forgotten majo\n",
      "rity are paying the price of bureaucratic Britain.      | rity are paying the price of bureaucratic Britain.\n",
      "\" The government has announced plans to cut 100,00      | \"\n",
      "0 civil servants as part of its efficiency drive.       | The government has announced plans to cut 100,000 \n",
      "The Liberal Democrats have said they would cut the      | civil servants as part of its efficiency drive. Bu\n",
      " number of Whitehall departments to make sure mone      | t Chief Secretary to the Treasury Paul Boateng att\n",
      "y reaches frontline services.                           | acked the Tory plans. \"The Conservatives are commi\n",
      "                                                        | tted to cutting Labour's public spending plans by \n",
      "                                                        | a massive £35 billion,\" he said. \"Cuts on this sca\n",
      "                                                        | le cannot be found from cutting 'bureaucracy' but \n",
      "                                                        | would require massive cuts to front-line public se\n",
      "                                                        | rvices such as schools, hospitals and the police.\"\n",
      "                                                        |  The Liberal Democrats have said they would cut th\n",
      "                                                        | e number of Whitehall departments to make sure mon\n",
      "                                                        | ey reaches frontline services.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = 0.8 # Threshold for similarity\n",
    "\n",
    "sorted_candidate_pairs = sorted(candidate_pairs, key=lambda x: CompareSets.jaccard(shingled_docs[x[0]], shingled_docs[x[1]]), reverse=True)\n",
    "for x, y in sorted_candidate_pairs:\n",
    "    if CompareSets.jaccard(shingled_docs[x], shingled_docs[y]) >= T:\n",
    "        print(f\"Shingle Similarity: {CompareSets.jaccard(shingled_docs[x], shingled_docs[y]):.4f}\")\n",
    "        print(f\"Signature Similarity: {CompareSignatures.jaccard(sig_matrix[x], sig_matrix[y]):.4f}\")\n",
    "        view_documents(docs, x, y, max_width=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
